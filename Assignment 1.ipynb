{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "5lDq2SIuGxny"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow.keras.models'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13024/3743750668.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLayerNormalization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGRU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.models'"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import os\n",
        "import gensim\n",
        "import scipy\n",
        "import numpy as np\n",
        "import collections\n",
        "import pandas as pd\n",
        "# from tqdm \n",
        "import tqdm\n",
        "from operator import add\n",
        "import  tensorflow as tf\n",
        "from collections import OrderedDict\n",
        "from utils_ import utility\n",
        "from urllib import request\n",
        "from itertools import chain\n",
        "from functools import reduce\n",
        "import gensim.downloader as gloader\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, LayerNormalization, LSTM, Dense, Bidirectional, GRU, Embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Error parsing requirements for google-pasta: [Errno 2] No such file or directory: 'c:\\\\users\\\\jyoti\\\\anaconda3\\\\envs\\\\yourenvname\\\\lib\\\\site-packages\\\\google_pasta-0.2.0.dist-info\\\\METADATA'\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\jyoti\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\jyoti\\anaconda3\\envs\\yourenvname\\lib\\site-packages (3.6.5)\n",
            "Requirement already satisfied: click in c:\\users\\jyoti\\anaconda3\\envs\\yourenvname\\lib\\site-packages (from nltk) (8.0.1)\n",
            "Requirement already satisfied: joblib in c:\\users\\jyoti\\anaconda3\\envs\\yourenvname\\lib\\site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jyoti\\anaconda3\\envs\\yourenvname\\lib\\site-packages (from nltk) (2021.8.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\jyoti\\anaconda3\\envs\\yourenvname\\lib\\site-packages (from nltk) (4.62.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\jyoti\\anaconda3\\envs\\yourenvname\\lib\\site-packages (from click->nltk) (0.4.4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB5XjvfCG58x",
        "outputId": "1beeda66-69d5-4ea2-8011-cef0c55e9c72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current work directory: c:\\Users\\jyoti\\Desktop\\Nlp_project\n",
            "Cleaned\n"
          ]
        }
      ],
      "source": [
        "utility.cleaning()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current work directory: c:\\Users\\jyoti\\Desktop\\Nlp_project\n",
            "c:\\Users\\jyoti\\Desktop\\Nlp_project\\Datasets\\dependency_treebank.zip\n",
            "c:\\Users\\jyoti\\Desktop\\Nlp_project\\Datasets\\dependency_treebank.zip\n",
            "Downloading dataset...\n",
            "Download complete!\n",
            "Extracting dataset... (it may take a while...)\n",
            "<zipfile.ZipFile filename='c:\\\\Users\\\\jyoti\\\\Desktop\\\\Nlp_project\\\\Datasets\\\\dependency_treebank.zip' mode='r'>\n",
            "Extraction completed!\n"
          ]
        }
      ],
      "source": [
        "url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
        "Datasets=\"Datasets\"\n",
        "dependency_treebank=\"dependency_treebank.zip\"\n",
        "utility.downloadDataSet(url,Datasets,dependency_treebank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "MBm1qo-VG_d2"
      },
      "outputs": [],
      "source": [
        "def splitting(i):\n",
        "    if i <= 100:\n",
        "        return 'train'\n",
        "    elif i <= 150:\n",
        "        return 'val'\n",
        "    else:\n",
        "        return 'test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "mLbwcKa6HBTz"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"dependency_treebank\"\n",
        "debug = True\n",
        "\n",
        "folder = os.path.join(os.getcwd(), \"Datasets\", dataset_name)\n",
        "files=os.listdir(folder)\n",
        "files.sort()\n",
        "dataframe_rows = {'train': [], 'val':[], 'test':[]}\n",
        "\n",
        "\n",
        "for i, filename in enumerate(files):\n",
        "  file_path = os.path.join(folder, filename)\n",
        "  try:\n",
        "        if os.path.isfile(file_path):\n",
        "              # open the file\n",
        "              list1, list2 = [], []\n",
        "              with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "              # read it and extract informations\n",
        "                # text = text_file.read()\n",
        "                for line in text_file:\n",
        "                    tokens = line.split(\"\\t\")[:2]\n",
        "                    # print(tokens)\n",
        "                    if len(tokens) == 2:\n",
        "                          list1.append(tokens[0])\n",
        "                          list2.append(tokens[1])\n",
        "                    if tokens[0] == '.' and splitting(i) != 'test':\n",
        "                          dataframe_rows[splitting(i)].append({\"tokens\": list1, \"tags\": list2})\n",
        "                          #print(dataframe_rows[:6])\n",
        "                          list1, list2 = [], []\n",
        "                if splitting(i) == 'test':\n",
        "                    dataframe_rows[splitting(i)].append({\"tokens\": list1, \"tags\": list2})\n",
        "                    list1, list2 = [], []\n",
        "  except Exception as e:\n",
        "        print('Failed to process %s. Reason: %s' % (file_path, e))\n",
        "        sys.exit(0)\n",
        "\n",
        "df_train = pd.DataFrame(dataframe_rows['train'])\n",
        "df_val = pd.DataFrame(dataframe_rows['val'])\n",
        "df_test = pd.DataFrame(dataframe_rows['test'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "0p9Zg2oAHRwn"
      },
      "outputs": [],
      "source": [
        "df_train['split']='train'\n",
        "df_test['split']='test'\n",
        "df_val['split']='val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "jAKAIgIfHG8W",
        "outputId": "585ead40-fa46-469d-dc86-b77343578b21"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>tags</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Pierre, Vinken, ,, 61, years, old, ,, will, j...</td>\n",
              "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Mr., Vinken, is, chairman, of, Elsevier, N.V....</td>\n",
              "      <td>[NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Rudolph, Agnew, ,, 55, years, old, and, forme...</td>\n",
              "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
              "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[The, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
              "      <td>[DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3243</th>\n",
              "      <td>[The, action, followed, by, one, day, an, Inte...</td>\n",
              "      <td>[DT, NN, VBN, IN, CD, NN, DT, NNP, NN, IN, PRP...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3244</th>\n",
              "      <td>[In, New, York, Stock, Exchange, composite, tr...</td>\n",
              "      <td>[IN, NNP, NNP, NNP, NNP, JJ, NN, NN, ,, NNP, N...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3245</th>\n",
              "      <td>[Mr., Edelman, declined, to, specify, what, pr...</td>\n",
              "      <td>[NNP, NNP, VBD, TO, VB, WP, VBD, DT, JJ, NNS, ...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3246</th>\n",
              "      <td>['', He, added, ,, ``, This, has, nothing, to,...</td>\n",
              "      <td>['', PRP, VBD, ,, ``, DT, VBZ, NN, TO, VB, IN,...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3247</th>\n",
              "      <td>['', But, Mr., Ackerman, said, the, buy-back, ...</td>\n",
              "      <td>['', CC, NNP, NNP, VBD, DT, NN, ,, CC, DT, JJ,...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3248 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 tokens  \\\n",
              "0     [Pierre, Vinken, ,, 61, years, old, ,, will, j...   \n",
              "1     [Mr., Vinken, is, chairman, of, Elsevier, N.V....   \n",
              "2     [Rudolph, Agnew, ,, 55, years, old, and, forme...   \n",
              "3     [A, form, of, asbestos, once, used, to, make, ...   \n",
              "4     [The, asbestos, fiber, ,, crocidolite, ,, is, ...   \n",
              "...                                                 ...   \n",
              "3243  [The, action, followed, by, one, day, an, Inte...   \n",
              "3244  [In, New, York, Stock, Exchange, composite, tr...   \n",
              "3245  [Mr., Edelman, declined, to, specify, what, pr...   \n",
              "3246  ['', He, added, ,, ``, This, has, nothing, to,...   \n",
              "3247  ['', But, Mr., Ackerman, said, the, buy-back, ...   \n",
              "\n",
              "                                                   tags  split  \n",
              "0     [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...  train  \n",
              "1     [NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...  train  \n",
              "2     [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...  train  \n",
              "3     [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...  train  \n",
              "4     [DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...  train  \n",
              "...                                                 ...    ...  \n",
              "3243  [DT, NN, VBN, IN, CD, NN, DT, NNP, NN, IN, PRP...    val  \n",
              "3244  [IN, NNP, NNP, NNP, NNP, JJ, NN, NN, ,, NNP, N...    val  \n",
              "3245  [NNP, NNP, VBD, TO, VB, WP, VBD, DT, JJ, NNS, ...    val  \n",
              "3246  ['', PRP, VBD, ,, ``, DT, VBZ, NN, TO, VB, IN,...    val  \n",
              "3247  ['', CC, NNP, NNP, VBD, DT, NN, ,, CC, DT, JJ,...    val  \n",
              "\n",
              "[3248 rows x 3 columns]"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.concat([df_train,df_test,df_val], ignore_index=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ferzyl9AHLLE",
        "outputId": "b55f5af2-56f0-4a85-beb5-62325c6529fd"
      },
      "outputs": [],
      "source": [
        "PAD, UNK, = '<pad>', '<unk>'\n",
        "#<pad> all sequences should have same length. If the sequence is short then it will be padded\n",
        "#<unk> means unknown token, used to replace rare word that didn't fir into the vocabulary\n",
        "t2i_train, i2t = utility.build_vocabulary(df.loc[df['split'] == 'train', 'tags'], [PAD, UNK])\n",
        "w2i_train, _ = utility.build_vocabulary(df.loc[df['split'] == 'train', 'tokens'])\n",
        "# print(len(t2i_train))\n",
        "# print(len(w2i_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5V6ur94HYpd",
        "outputId": "c7ffe507-b2ef-44f4-c918-b820231bf6ff"
      },
      "outputs": [],
      "source": [
        "t2i_val, i2t_val = utility.build_vocabulary(df.loc[df['split'] == 'val', 'tags'], [PAD, UNK])\n",
        "#print(df.loc[df['split'] == 'train', 'tags'])\n",
        "w2i_val, _ = utility.build_vocabulary(df.loc[df['split'] == 'val', 'tokens'])\n",
        "# print(t2i_val)\n",
        "# print(w2i_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnKR7V0vHaWo",
        "outputId": "78d9254c-09c2-45da-bab5-266ad753780e"
      },
      "outputs": [],
      "source": [
        "t2i_test, i2t_test = utility.build_vocabulary(df.loc[df['split'] == 'test', 'tags'], [PAD, UNK])\n",
        "#print(df.loc[df['split'] == 'train', 'tags'])\n",
        "w2i_test, _ = utility.build_vocabulary(df.loc[df['split'] == 'test', 'tokens'])\n",
        "# print(t2i_test)\n",
        "# print(w2i_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7IqZgY7Hc7E",
        "outputId": "5abe4ed9-9849-4f3c-a0c2-1e10c2892f24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('departments', 7478),\n",
              " ('17.3', 7477),\n",
              " ('bounce', 7476),\n",
              " ('leadership', 7475),\n",
              " ('flights', 7474),\n",
              " ('airline', 7473),\n",
              " ('27.1', 7472),\n",
              " ('anti-drug', 7471),\n",
              " ('supplemental', 7470),\n",
              " ('waived', 7469)]"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_fd=nltk.probability.FreqDist(w2i_train)\n",
        "tokens_fd.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "l_9w1tEVHhON"
      },
      "outputs": [],
      "source": [
        "embedding_dimension=100\n",
        "\n",
        "embedding_model = utility.load_embedding_model('glove',embedding_dimension)\n",
        "\n",
        "# allows \"PAD\" to have index zero, crucial for consistency\n",
        "PAD, UNK, = '<pad>', '<unk>'\n",
        "pad_unk_vec = np.zeros((2, embedding_dimension)) # pad\n",
        "pad_unk_vec[1] = np.random.rand(embedding_dimension) # unk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>tags</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Beauty, Takes, Backseat, To, Safety, on, Brid...</td>\n",
              "      <td>[NN, VBZ, NN, TO, NNP, IN, NNPS, NN, VBZ, IN, ...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[But, there, 's, disagreement, over, how, to, ...</td>\n",
              "      <td>[CC, EX, VBZ, NN, IN, WRB, TO, VB, PRP, .]</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Highway, officials, insist, the, ornamental, ...</td>\n",
              "      <td>[NN, NNS, VBP, DT, JJ, NNS, IN, JJR, NNS, VBP,...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[But, other, people, do, n't, want, to, lose, ...</td>\n",
              "      <td>[CC, JJ, NNS, VBP, RB, VB, TO, VB, DT, NNS, PO...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[``, The, primary, purpose, of, a, railing, is...</td>\n",
              "      <td>[``, DT, JJ, NN, IN, DT, NN, VBZ, TO, VB, DT, ...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1237</th>\n",
              "      <td>[The, action, followed, by, one, day, an, Inte...</td>\n",
              "      <td>[DT, NN, VBN, IN, CD, NN, DT, NNP, NN, IN, PRP...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1238</th>\n",
              "      <td>[In, New, York, Stock, Exchange, composite, tr...</td>\n",
              "      <td>[IN, NNP, NNP, NNP, NNP, JJ, NN, NN, ,, NNP, N...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1239</th>\n",
              "      <td>[Mr., Edelman, declined, to, specify, what, pr...</td>\n",
              "      <td>[NNP, NNP, VBD, TO, VB, WP, VBD, DT, JJ, NNS, ...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1240</th>\n",
              "      <td>['', He, added, ,, ``, This, has, nothing, to,...</td>\n",
              "      <td>['', PRP, VBD, ,, ``, DT, VBZ, NN, TO, VB, IN,...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1241</th>\n",
              "      <td>['', But, Mr., Ackerman, said, the, buy-back, ...</td>\n",
              "      <td>['', CC, NNP, NNP, VBD, DT, NN, ,, CC, DT, JJ,...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1242 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 tokens  \\\n",
              "0     [Beauty, Takes, Backseat, To, Safety, on, Brid...   \n",
              "1     [But, there, 's, disagreement, over, how, to, ...   \n",
              "2     [Highway, officials, insist, the, ornamental, ...   \n",
              "3     [But, other, people, do, n't, want, to, lose, ...   \n",
              "4     [``, The, primary, purpose, of, a, railing, is...   \n",
              "...                                                 ...   \n",
              "1237  [The, action, followed, by, one, day, an, Inte...   \n",
              "1238  [In, New, York, Stock, Exchange, composite, tr...   \n",
              "1239  [Mr., Edelman, declined, to, specify, what, pr...   \n",
              "1240  ['', He, added, ,, ``, This, has, nothing, to,...   \n",
              "1241  ['', But, Mr., Ackerman, said, the, buy-back, ...   \n",
              "\n",
              "                                                   tags split  \n",
              "0     [NN, VBZ, NN, TO, NNP, IN, NNPS, NN, VBZ, IN, ...   val  \n",
              "1            [CC, EX, VBZ, NN, IN, WRB, TO, VB, PRP, .]   val  \n",
              "2     [NN, NNS, VBP, DT, JJ, NNS, IN, JJR, NNS, VBP,...   val  \n",
              "3     [CC, JJ, NNS, VBP, RB, VB, TO, VB, DT, NNS, PO...   val  \n",
              "4     [``, DT, JJ, NN, IN, DT, NN, VBZ, TO, VB, DT, ...   val  \n",
              "...                                                 ...   ...  \n",
              "1237  [DT, NN, VBN, IN, CD, NN, DT, NNP, NN, IN, PRP...   val  \n",
              "1238  [IN, NNP, NNP, NNP, NNP, JJ, NN, NN, ,, NNP, N...   val  \n",
              "1239  [NNP, NNP, VBD, TO, VB, WP, VBD, DT, JJ, NNS, ...   val  \n",
              "1240  ['', PRP, VBD, ,, ``, DT, VBZ, NN, TO, VB, IN,...   val  \n",
              "1241  ['', CC, NNP, NNP, VBD, DT, NN, ,, CC, DT, JJ,...   val  \n",
              "\n",
              "[1242 rows x 3 columns]"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train=df.loc[df['split']=='train']\n",
        "#df_train\n",
        "df_val=df.loc[df['split']=='val']\n",
        "#df_val\n",
        "df_test=df.loc[df['split']=='test']\n",
        "#df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Y6HK1obOHxml"
      },
      "outputs": [],
      "source": [
        "def text_2_categorical(df, w2i, t2i):\n",
        "    \"\"\"Converts DataFrame words to categorical for use in the RNNs\"\"\"\n",
        "    df['tokens'] = df['tokens'].map(lambda s: [w2i.get(w.lower().strip(), w2i[UNK]) for w in s])\n",
        "    df['tags'] = df['tags'].map(lambda s: [t2i.get(w.lower().strip()) for w in s])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "6M8wgmBug-sf",
        "outputId": "d106a0ba-308b-498e-d171-ef6c0a8897bc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>tags</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Pierre, Vinken, ,, 61, years, old, ,, will, j...</td>\n",
              "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Mr., Vinken, is, chairman, of, Elsevier, N.V....</td>\n",
              "      <td>[NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Rudolph, Agnew, ,, 55, years, old, and, forme...</td>\n",
              "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
              "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[The, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
              "      <td>[DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3243</th>\n",
              "      <td>[The, action, followed, by, one, day, an, Inte...</td>\n",
              "      <td>[DT, NN, VBN, IN, CD, NN, DT, NNP, NN, IN, PRP...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3244</th>\n",
              "      <td>[In, New, York, Stock, Exchange, composite, tr...</td>\n",
              "      <td>[IN, NNP, NNP, NNP, NNP, JJ, NN, NN, ,, NNP, N...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3245</th>\n",
              "      <td>[Mr., Edelman, declined, to, specify, what, pr...</td>\n",
              "      <td>[NNP, NNP, VBD, TO, VB, WP, VBD, DT, JJ, NNS, ...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3246</th>\n",
              "      <td>['', He, added, ,, ``, This, has, nothing, to,...</td>\n",
              "      <td>['', PRP, VBD, ,, ``, DT, VBZ, NN, TO, VB, IN,...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3247</th>\n",
              "      <td>['', But, Mr., Ackerman, said, the, buy-back, ...</td>\n",
              "      <td>['', CC, NNP, NNP, VBD, DT, NN, ,, CC, DT, JJ,...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3248 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 tokens  \\\n",
              "0     [Pierre, Vinken, ,, 61, years, old, ,, will, j...   \n",
              "1     [Mr., Vinken, is, chairman, of, Elsevier, N.V....   \n",
              "2     [Rudolph, Agnew, ,, 55, years, old, and, forme...   \n",
              "3     [A, form, of, asbestos, once, used, to, make, ...   \n",
              "4     [The, asbestos, fiber, ,, crocidolite, ,, is, ...   \n",
              "...                                                 ...   \n",
              "3243  [The, action, followed, by, one, day, an, Inte...   \n",
              "3244  [In, New, York, Stock, Exchange, composite, tr...   \n",
              "3245  [Mr., Edelman, declined, to, specify, what, pr...   \n",
              "3246  ['', He, added, ,, ``, This, has, nothing, to,...   \n",
              "3247  ['', But, Mr., Ackerman, said, the, buy-back, ...   \n",
              "\n",
              "                                                   tags  split  \n",
              "0     [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...  train  \n",
              "1     [NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...  train  \n",
              "2     [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...  train  \n",
              "3     [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...  train  \n",
              "4     [DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...  train  \n",
              "...                                                 ...    ...  \n",
              "3243  [DT, NN, VBN, IN, CD, NN, DT, NNP, NN, IN, PRP...    val  \n",
              "3244  [IN, NNP, NNP, NNP, NNP, JJ, NN, NN, ,, NNP, N...    val  \n",
              "3245  [NNP, NNP, VBD, TO, VB, WP, VBD, DT, JJ, NNS, ...    val  \n",
              "3246  ['', PRP, VBD, ,, ``, DT, VBZ, NN, TO, VB, IN,...    val  \n",
              "3247  ['', CC, NNP, NNP, VBD, DT, NN, ,, CC, DT, JJ,...    val  \n",
              "\n",
              "[3248 rows x 3 columns]"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "82LU_MvZ3f1D"
      },
      "outputs": [],
      "source": [
        "# pos tag vocabulary\n",
        "p2i, i2p = utility.build_vocabulary(df_train['tags'], [PAD, UNK])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "_gtICaDoH7Ko"
      },
      "outputs": [],
      "source": [
        "#Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "voc1 = {e:i for i,e in enumerate(chain([PAD, UNK], embedding_model.vocab.keys()))}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTq15rxUTFSA"
      },
      "source": [
        "##Build traning vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZSOCcmEH9MO",
        "outputId": "41a542b7-812e-4396-ef81-fe288e92afb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 362 oov words: 4.8%\n"
          ]
        }
      ],
      "source": [
        "#Now let's build the training vocabulary\n",
        "#Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "\n",
        "w2i_train, i2w_train = utility.build_vocabulary(df_train['tokens'])\n",
        "voc2= utility.combine_vocabularies(voc1, w2i_train)\n",
        "oov1=utility.check_OOV_terms(voc1, w2i_train)\n",
        "print(f\"Found {len(oov1)} oov words: {utility.getPercent(len(oov1), len(w2i_train)):.2}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqdX9TEcVc1x",
        "outputId": "3d78360e-5fd4-4593-a2b4-50c695037bc6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\jyoti\\AppData\\Local\\Temp/ipykernel_13024/2031027438.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['tokens'] = df['tokens'].map(lambda s: [w2i.get(w.lower().strip(), w2i[UNK]) for w in s])\n",
            "C:\\Users\\jyoti\\AppData\\Local\\Temp/ipykernel_13024/2031027438.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['tags'] = df['tags'].map(lambda s: [t2i.get(w.lower().strip()) for w in s])\n"
          ]
        }
      ],
      "source": [
        "text_2_categorical(df_train, voc2, t2i_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "7DTXEA5irXJt",
        "outputId": "b63b4df3-b1f6-48cb-a3ad-a955356f8e7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1958/1958 [00:03<00:00, 627.16it/s]\n"
          ]
        }
      ],
      "source": [
        "inv2 = {v:k for k,v in voc2.items()}\n",
        "co_occ1 = utility.co_occurrence_count(df_train, inv2, sparse=True)\n",
        "emb=utility.embedd_OOV_terms(embedding_model, oov1, co_occ1, voc2, inv2,embedding_dimension,rnd_OOV = False)\n",
        "embedding_matrix_v2 = np.concatenate((pad_unk_vec, embedding_model.vectors.copy()))\n",
        "#print(embedding_matrix_v2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w08qgVAe3-tW"
      },
      "source": [
        "##Building Validation Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "MVz2oEzA4Cki"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 189 oov words: 3.5%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\jyoti\\AppData\\Local\\Temp/ipykernel_13024/2031027438.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['tokens'] = df['tokens'].map(lambda s: [w2i.get(w.lower().strip(), w2i[UNK]) for w in s])\n",
            "C:\\Users\\jyoti\\AppData\\Local\\Temp/ipykernel_13024/2031027438.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['tags'] = df['tags'].map(lambda s: [t2i.get(w.lower().strip()) for w in s])\n"
          ]
        }
      ],
      "source": [
        "w2i_val, _ = utility.build_vocabulary(df_val['tokens'])\n",
        "voc3= utility.combine_vocabularies(voc2, w2i_val)\n",
        "oov2=utility.check_OOV_terms(voc2, w2i_val)\n",
        "print(f\"Found {len(oov2)} oov words: {utility.getPercent(len(oov2), len(w2i_val)):.2}%\\n\")\n",
        "text_2_categorical(df_val, voc3, t2i_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1242/1242 [00:01<00:00, 634.80it/s]\n"
          ]
        }
      ],
      "source": [
        "inv3 = {v:k for k,v in voc3.items()}\n",
        "co_occ2 = utility.co_occurrence_count(df_val, inv3, sparse=True)\n",
        "embeding_model = utility.embedd_OOV_terms(embedding_model, oov2, co_occ2, voc3, inv3,embedding_dimension,rnd_OOV = False)\n",
        "embedding_matrix_v3 = np.concatenate((pad_unk_vec, embedding_model.vectors.copy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnUX6xaLTQ_5"
      },
      "source": [
        "##Build test vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "ZyA4PQPA5N4C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 125 oov words: 3.7%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\jyoti\\AppData\\Local\\Temp/ipykernel_13024/2031027438.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['tokens'] = df['tokens'].map(lambda s: [w2i.get(w.lower().strip(), w2i[UNK]) for w in s])\n",
            "C:\\Users\\jyoti\\AppData\\Local\\Temp/ipykernel_13024/2031027438.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['tags'] = df['tags'].map(lambda s: [t2i.get(w.lower().strip()) for w in s])\n"
          ]
        }
      ],
      "source": [
        "w2i_test, _ = utility.build_vocabulary(df_test['tokens'])\n",
        "voc4= utility.combine_vocabularies(voc3, w2i_test)\n",
        "oov3=utility.check_OOV_terms(voc3, w2i_test)\n",
        "print(f\"Found {len(oov3)} oov words: {utility.getPercent(len(oov3),len(w2i_test)):.2}%\")\n",
        "text_2_categorical(df_test, voc4, t2i_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:00<00:00, 52.53it/s]\n"
          ]
        }
      ],
      "source": [
        "inv4 = {v:k for k,v in voc4.items()}\n",
        "co_occ3 = utility.co_occurrence_count(df_test, inv4, sparse=True)\n",
        "embeding_model = utility.embedd_OOV_terms(embedding_model, oov3, co_occ3, voc4, inv4,embedding_dimension,rnd_OOV = False)\n",
        "embedding_matrix_v4 = np.concatenate((pad_unk_vec, embedding_model.vectors.copy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "Oo1mhV2nuV5j"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1958</th>\n",
              "      <td>[2506, 1157, 1163, 1170, 18, 22, 2088, 49, 82,...</td>\n",
              "      <td>[5, 5, 16, 5, 32, 20, 32, 31, 38, 11, 2, 6, 5,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1959</th>\n",
              "      <td>[16859, 148, 1020, 295, 2985, 1479, 1464, 12, ...</td>\n",
              "      <td>[5, 5, 5, 32, 11, 2, 12, 6, 9, 11, 2, 32, 5, 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1960</th>\n",
              "      <td>[7620, 953, 1020, 18, 22, 613, 470, 15, 203, 5...</td>\n",
              "      <td>[5, 5, 5, 32, 20, 32, 12, 6, 11, 6, 31, 11, 2,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1961</th>\n",
              "      <td>[87, 3193, 3083, 470, 6494, 15, 5879, 14, 1344...</td>\n",
              "      <td>[2, 2, 12, 12, 32, 6, 12, 6, 5, 24, 9, 11, 2, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1962</th>\n",
              "      <td>[2, 3794, 5, 115381, 3, 51, 3273, 6, 598, 9, 7...</td>\n",
              "      <td>[9, 2, 6, 5, 24, 6, 23, 4, 14, 9, 2, 6, 9, 2, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1963</th>\n",
              "      <td>[2065, 6711, 1170, 18, 14, 73138, 785, 821, 13...</td>\n",
              "      <td>[5, 5, 5, 32, 6, 5, 5, 5, 5, 5, 3, 15, 23, 9, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1964</th>\n",
              "      <td>[10853, 4170, 5784, 18, 787, 400576, 3142, 21,...</td>\n",
              "      <td>[5, 5, 5, 32, 5, 5, 32, 6, 11, 2, 4, 14, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1965</th>\n",
              "      <td>[38582, 4076, 1020, 3, 9, 2885, 5, 400580, 776...</td>\n",
              "      <td>[5, 5, 5, 24, 9, 2, 6, 5, 5, 5, 10, 5, 5, 5, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1966</th>\n",
              "      <td>[102798, 1020, 295, 9, 210377, 1052, 868, 5, 8...</td>\n",
              "      <td>[5, 5, 32, 9, 2, 11, 2, 6, 38, 28, 28, 24, 16,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1967</th>\n",
              "      <td>[400584, 1020, 18, 22, 2872, 6, 257, 9, 210377...</td>\n",
              "      <td>[5, 5, 32, 20, 3, 4, 14, 9, 2, 11, 2, 6, 38, 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1968</th>\n",
              "      <td>[9, 1494, 5, 7066, 19, 2, 995, 253, 935, 9837,...</td>\n",
              "      <td>[9, 2, 6, 2, 6, 9, 8, 11, 12, 32, 9, 2, 2, 10,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1969</th>\n",
              "      <td>[291006, 733, 18, 22, 45, 7039, 49, 291006, 63...</td>\n",
              "      <td>[5, 5, 32, 20, 30, 14, 31, 5, 5, 7, 5, 2, 6, 9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1970</th>\n",
              "      <td>[7700, 2824, 1170, 3, 363, 11, 38792, 571, 273...</td>\n",
              "      <td>[5, 5, 5, 24, 5, 10, 11, 2, 2, 24, 32, 5, 6, 9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1971</th>\n",
              "      <td>[3457, 495, 1020, 3, 52, 198, 3, 18, 22, 1114,...</td>\n",
              "      <td>[5, 5, 5, 24, 5, 5, 24, 32, 20, 32, 4, 14, 9, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1972</th>\n",
              "      <td>[176855, 733, 3, 2090, 23, 9, 1191, 1962, 7, 7...</td>\n",
              "      <td>[5, 5, 24, 15, 6, 9, 2, 2, 16, 11, 11, 12, 24,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1973</th>\n",
              "      <td>[34452, 148, 733, 3, 400604, 5626, 3, 7893, 3,...</td>\n",
              "      <td>[5, 5, 5, 24, 5, 5, 24, 5, 24, 23, 9, 11, 2, 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1974</th>\n",
              "      <td>[18916, 123, 1020, 18, 22, 921, 15694, 546, 5,...</td>\n",
              "      <td>[5, 5, 5, 32, 20, 32, 28, 12, 6, 31, 11, 2, 4,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1975</th>\n",
              "      <td>[400156, 131, 5784, 3, 9, 74904, 3, 400609, 42...</td>\n",
              "      <td>[5, 5, 5, 24, 9, 11, 24, 11, 11, 2, 24, 32, 31...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1976</th>\n",
              "      <td>[9453, 4569, 1332, 733, 18, 1158, 673, 19, 2, ...</td>\n",
              "      <td>[5, 5, 5, 5, 32, 11, 12, 6, 9, 2, 6, 28, 11, 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1977</th>\n",
              "      <td>[1460, 349, 3664, 1163, 2266, 642, 249, 31, 82...</td>\n",
              "      <td>[5, 5, 5, 16, 5, 5, 32, 9, 38, 28, 28, 2, 6, 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1978</th>\n",
              "      <td>[60, 5, 455, 233, 1020, 18, 22, 1317, 49, 4076...</td>\n",
              "      <td>[5, 6, 5, 5, 5, 32, 20, 32, 31, 2, 6, 5, 5, 5,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1979</th>\n",
              "      <td>[161181, 2610, 733, 3, 9, 444, 128311, 6701, 2...</td>\n",
              "      <td>[5, 7, 5, 24, 9, 22, 11, 2, 2, 39, 2, 11, 6, 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980</th>\n",
              "      <td>[13834, 845, 9, 4907, 19, 2, 50294, 7, 18, 615...</td>\n",
              "      <td>[5, 32, 9, 2, 6, 9, 7, 16, 32, 12, 32, 15, 17,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1981</th>\n",
              "      <td>[4504, 1020, 1317, 49, 7384, 903, 12, 2468, 24...</td>\n",
              "      <td>[5, 5, 32, 31, 2, 2, 6, 5, 7, 5, 5, 24, 6, 5, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1982</th>\n",
              "      <td>[17294, 1020, 33, 156, 1244, 8, 49, 56311, 116...</td>\n",
              "      <td>[5, 5, 3, 15, 12, 6, 31, 5, 16, 5, 11, 2, 12, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1983</th>\n",
              "      <td>[60908, 3046, 3, 9, 428, 5436, 2739, 5, 918, 6...</td>\n",
              "      <td>[5, 5, 24, 9, 11, 15, 2, 6, 2, 2, 24, 32, 9, 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1984</th>\n",
              "      <td>[253, 554, 1660, 6, 32, 26267, 140, 3, 995, 68...</td>\n",
              "      <td>[11, 2, 3, 4, 14, 23, 6, 24, 8, 12, 13, 17, 2,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985</th>\n",
              "      <td>[400250, 68941, 3, 31, 3205, 1658, 7, 645, 629...</td>\n",
              "      <td>[2, 12, 24, 9, 11, 7, 16, 5, 5, 2, 24, 9, 5, 5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1986</th>\n",
              "      <td>[13937, 1066, 733, 3, 4806, 3, 9218, 3, 18, 22...</td>\n",
              "      <td>[5, 5, 5, 24, 5, 24, 5, 24, 32, 20, 32, 15, 5,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1987</th>\n",
              "      <td>[2, 274, 468, 11, 3174, 5, 11258, 1162, 6, 9, ...</td>\n",
              "      <td>[9, 5, 2, 10, 2, 6, 5, 5, 4, 9, 2, 6, 9, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1988</th>\n",
              "      <td>[111355, 5987, 18, 22, 739, 6, 5540, 3842, 264...</td>\n",
              "      <td>[5, 5, 32, 20, 32, 4, 14, 28, 2, 6, 5, 5, 6, 9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1989</th>\n",
              "      <td>[2, 101, 148, 314, 629, 1103, 3597, 12064, 126...</td>\n",
              "      <td>[9, 5, 5, 5, 5, 32, 11, 12, 6, 9, 5, 11, 2, 6,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1990</th>\n",
              "      <td>[38398, 1170, 18, 22, 45, 903, 31, 201, 2496, ...</td>\n",
              "      <td>[5, 5, 32, 20, 30, 14, 9, 11, 2, 2, 4, 22, 11,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1991</th>\n",
              "      <td>[400652, 15233, 4, 400046, 11370, 3, 12135, 40...</td>\n",
              "      <td>[5, 5, 17, 40, 5, 24, 5, 41, 33, 5, 5, 24, 23,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1992</th>\n",
              "      <td>[30, 6952, 13264, 29, 400046, 1795, 13370, 3, ...</td>\n",
              "      <td>[25, 5, 5, 26, 40, 5, 5, 24, 28, 12, 24, 38, 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1993</th>\n",
              "      <td>[2, 5695, 80, 7, 226, 8014, 5, 9403, 12845, 40...</td>\n",
              "      <td>[9, 11, 2, 16, 11, 12, 6, 11, 2, 5, 5, 7, 5, 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1994</th>\n",
              "      <td>[49, 561, 6, 32, 2783, 16720, 3, 3784, 759, 10...</td>\n",
              "      <td>[31, 12, 4, 14, 15, 15, 24, 5, 5, 5, 32, 20, 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>[26372, 64147, 1170, 295, 9, 3619, 264, 1945, ...</td>\n",
              "      <td>[5, 5, 5, 32, 9, 28, 2, 2, 6, 2, 12, 24, 23, 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>[339, 1725, 1270, 295, 6208, 1271, 7510, 496, ...</td>\n",
              "      <td>[12, 15, 6, 32, 11, 2, 22, 11, 6, 9, 2, 6, 12,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>[60, 1149, 1020, 18, 22, 1317, 49, 82, 75267, ...</td>\n",
              "      <td>[5, 5, 5, 32, 20, 32, 31, 38, 28, 28, 11, 2, 6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>[2, 386, 400665, 23275, 23, 2, 1334, 356, 16, ...</td>\n",
              "      <td>[9, 2, 11, 2, 6, 9, 5, 5, 3, 23, 11, 12, 6, 9,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>[34803, 1020, 5535, 57, 2702, 5, 49, 2937, 333...</td>\n",
              "      <td>[5, 5, 32, 28, 12, 6, 31, 5, 2, 6, 11, 12, 16,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000</th>\n",
              "      <td>[74898, 1020, 2597, 9, 1023, 264, 1562, 8, 210...</td>\n",
              "      <td>[5, 5, 32, 9, 28, 2, 2, 6, 2, 2, 24, 6, 15, 2,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2001</th>\n",
              "      <td>[281, 3698, 11697, 3, 802, 3, 3137, 618, 856, ...</td>\n",
              "      <td>[5, 5, 5, 24, 28, 24, 22, 11, 2, 2, 16, 2, 11,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002</th>\n",
              "      <td>[6991, 2082, 507, 3, 856, 92, 7, 278, 397, 943...</td>\n",
              "      <td>[5, 5, 5, 24, 2, 2, 16, 2, 11, 2, 6, 9, 2, 12,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2003</th>\n",
              "      <td>[686, 2136, 400676, 3, 92, 5, 2429, 8306, 724,...</td>\n",
              "      <td>[5, 5, 5, 24, 2, 6, 2, 2, 2, 24, 32, 15, 4, 9,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2004</th>\n",
              "      <td>[57, 596, 400677, 1234, 18, 92, 274, 262, 72, ...</td>\n",
              "      <td>[28, 23, 2, 12, 32, 5, 5, 3, 22, 14, 9, 11, 2,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2005</th>\n",
              "      <td>[8299, 2610, 733, 18, 22, 754, 9, 3597, 549, 6...</td>\n",
              "      <td>[5, 7, 5, 32, 20, 32, 9, 11, 2, 4, 14, 28, 2, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 tokens  \\\n",
              "1958  [2506, 1157, 1163, 1170, 18, 22, 2088, 49, 82,...   \n",
              "1959  [16859, 148, 1020, 295, 2985, 1479, 1464, 12, ...   \n",
              "1960  [7620, 953, 1020, 18, 22, 613, 470, 15, 203, 5...   \n",
              "1961  [87, 3193, 3083, 470, 6494, 15, 5879, 14, 1344...   \n",
              "1962  [2, 3794, 5, 115381, 3, 51, 3273, 6, 598, 9, 7...   \n",
              "1963  [2065, 6711, 1170, 18, 14, 73138, 785, 821, 13...   \n",
              "1964  [10853, 4170, 5784, 18, 787, 400576, 3142, 21,...   \n",
              "1965  [38582, 4076, 1020, 3, 9, 2885, 5, 400580, 776...   \n",
              "1966  [102798, 1020, 295, 9, 210377, 1052, 868, 5, 8...   \n",
              "1967  [400584, 1020, 18, 22, 2872, 6, 257, 9, 210377...   \n",
              "1968  [9, 1494, 5, 7066, 19, 2, 995, 253, 935, 9837,...   \n",
              "1969  [291006, 733, 18, 22, 45, 7039, 49, 291006, 63...   \n",
              "1970  [7700, 2824, 1170, 3, 363, 11, 38792, 571, 273...   \n",
              "1971  [3457, 495, 1020, 3, 52, 198, 3, 18, 22, 1114,...   \n",
              "1972  [176855, 733, 3, 2090, 23, 9, 1191, 1962, 7, 7...   \n",
              "1973  [34452, 148, 733, 3, 400604, 5626, 3, 7893, 3,...   \n",
              "1974  [18916, 123, 1020, 18, 22, 921, 15694, 546, 5,...   \n",
              "1975  [400156, 131, 5784, 3, 9, 74904, 3, 400609, 42...   \n",
              "1976  [9453, 4569, 1332, 733, 18, 1158, 673, 19, 2, ...   \n",
              "1977  [1460, 349, 3664, 1163, 2266, 642, 249, 31, 82...   \n",
              "1978  [60, 5, 455, 233, 1020, 18, 22, 1317, 49, 4076...   \n",
              "1979  [161181, 2610, 733, 3, 9, 444, 128311, 6701, 2...   \n",
              "1980  [13834, 845, 9, 4907, 19, 2, 50294, 7, 18, 615...   \n",
              "1981  [4504, 1020, 1317, 49, 7384, 903, 12, 2468, 24...   \n",
              "1982  [17294, 1020, 33, 156, 1244, 8, 49, 56311, 116...   \n",
              "1983  [60908, 3046, 3, 9, 428, 5436, 2739, 5, 918, 6...   \n",
              "1984  [253, 554, 1660, 6, 32, 26267, 140, 3, 995, 68...   \n",
              "1985  [400250, 68941, 3, 31, 3205, 1658, 7, 645, 629...   \n",
              "1986  [13937, 1066, 733, 3, 4806, 3, 9218, 3, 18, 22...   \n",
              "1987  [2, 274, 468, 11, 3174, 5, 11258, 1162, 6, 9, ...   \n",
              "1988  [111355, 5987, 18, 22, 739, 6, 5540, 3842, 264...   \n",
              "1989  [2, 101, 148, 314, 629, 1103, 3597, 12064, 126...   \n",
              "1990  [38398, 1170, 18, 22, 45, 903, 31, 201, 2496, ...   \n",
              "1991  [400652, 15233, 4, 400046, 11370, 3, 12135, 40...   \n",
              "1992  [30, 6952, 13264, 29, 400046, 1795, 13370, 3, ...   \n",
              "1993  [2, 5695, 80, 7, 226, 8014, 5, 9403, 12845, 40...   \n",
              "1994  [49, 561, 6, 32, 2783, 16720, 3, 3784, 759, 10...   \n",
              "1995  [26372, 64147, 1170, 295, 9, 3619, 264, 1945, ...   \n",
              "1996  [339, 1725, 1270, 295, 6208, 1271, 7510, 496, ...   \n",
              "1997  [60, 1149, 1020, 18, 22, 1317, 49, 82, 75267, ...   \n",
              "1998  [2, 386, 400665, 23275, 23, 2, 1334, 356, 16, ...   \n",
              "1999  [34803, 1020, 5535, 57, 2702, 5, 49, 2937, 333...   \n",
              "2000  [74898, 1020, 2597, 9, 1023, 264, 1562, 8, 210...   \n",
              "2001  [281, 3698, 11697, 3, 802, 3, 3137, 618, 856, ...   \n",
              "2002  [6991, 2082, 507, 3, 856, 92, 7, 278, 397, 943...   \n",
              "2003  [686, 2136, 400676, 3, 92, 5, 2429, 8306, 724,...   \n",
              "2004  [57, 596, 400677, 1234, 18, 92, 274, 262, 72, ...   \n",
              "2005  [8299, 2610, 733, 18, 22, 754, 9, 3597, 549, 6...   \n",
              "\n",
              "                                                   tags  \n",
              "1958  [5, 5, 16, 5, 32, 20, 32, 31, 38, 11, 2, 6, 5,...  \n",
              "1959  [5, 5, 5, 32, 11, 2, 12, 6, 9, 11, 2, 32, 5, 2...  \n",
              "1960  [5, 5, 5, 32, 20, 32, 12, 6, 11, 6, 31, 11, 2,...  \n",
              "1961  [2, 2, 12, 12, 32, 6, 12, 6, 5, 24, 9, 11, 2, ...  \n",
              "1962  [9, 2, 6, 5, 24, 6, 23, 4, 14, 9, 2, 6, 9, 2, ...  \n",
              "1963  [5, 5, 5, 32, 6, 5, 5, 5, 5, 5, 3, 15, 23, 9, ...  \n",
              "1964  [5, 5, 5, 32, 5, 5, 32, 6, 11, 2, 4, 14, 11, 1...  \n",
              "1965  [5, 5, 5, 24, 9, 2, 6, 5, 5, 5, 10, 5, 5, 5, 1...  \n",
              "1966  [5, 5, 32, 9, 2, 11, 2, 6, 38, 28, 28, 24, 16,...  \n",
              "1967  [5, 5, 32, 20, 3, 4, 14, 9, 2, 11, 2, 6, 38, 2...  \n",
              "1968  [9, 2, 6, 2, 6, 9, 8, 11, 12, 32, 9, 2, 2, 10,...  \n",
              "1969  [5, 5, 32, 20, 30, 14, 31, 5, 5, 7, 5, 2, 6, 9...  \n",
              "1970  [5, 5, 5, 24, 5, 10, 11, 2, 2, 24, 32, 5, 6, 9...  \n",
              "1971  [5, 5, 5, 24, 5, 5, 24, 32, 20, 32, 4, 14, 9, ...  \n",
              "1972  [5, 5, 24, 15, 6, 9, 2, 2, 16, 11, 11, 12, 24,...  \n",
              "1973  [5, 5, 5, 24, 5, 5, 24, 5, 24, 23, 9, 11, 2, 2...  \n",
              "1974  [5, 5, 5, 32, 20, 32, 28, 12, 6, 31, 11, 2, 4,...  \n",
              "1975  [5, 5, 5, 24, 9, 11, 24, 11, 11, 2, 24, 32, 31...  \n",
              "1976  [5, 5, 5, 5, 32, 11, 12, 6, 9, 2, 6, 28, 11, 2...  \n",
              "1977  [5, 5, 5, 16, 5, 5, 32, 9, 38, 28, 28, 2, 6, 2...  \n",
              "1978  [5, 6, 5, 5, 5, 32, 20, 32, 31, 2, 6, 5, 5, 5,...  \n",
              "1979  [5, 7, 5, 24, 9, 22, 11, 2, 2, 39, 2, 11, 6, 3...  \n",
              "1980  [5, 32, 9, 2, 6, 9, 7, 16, 32, 12, 32, 15, 17,...  \n",
              "1981  [5, 5, 32, 31, 2, 2, 6, 5, 7, 5, 5, 24, 6, 5, ...  \n",
              "1982  [5, 5, 3, 15, 12, 6, 31, 5, 16, 5, 11, 2, 12, ...  \n",
              "1983  [5, 5, 24, 9, 11, 15, 2, 6, 2, 2, 24, 32, 9, 2...  \n",
              "1984  [11, 2, 3, 4, 14, 23, 6, 24, 8, 12, 13, 17, 2,...  \n",
              "1985  [2, 12, 24, 9, 11, 7, 16, 5, 5, 2, 24, 9, 5, 5...  \n",
              "1986  [5, 5, 5, 24, 5, 24, 5, 24, 32, 20, 32, 15, 5,...  \n",
              "1987  [9, 5, 2, 10, 2, 6, 5, 5, 4, 9, 2, 6, 9, 11, 1...  \n",
              "1988  [5, 5, 32, 20, 32, 4, 14, 28, 2, 6, 5, 5, 6, 9...  \n",
              "1989  [9, 5, 5, 5, 5, 32, 11, 12, 6, 9, 5, 11, 2, 6,...  \n",
              "1990  [5, 5, 32, 20, 30, 14, 9, 11, 2, 2, 4, 22, 11,...  \n",
              "1991  [5, 5, 17, 40, 5, 24, 5, 41, 33, 5, 5, 24, 23,...  \n",
              "1992  [25, 5, 5, 26, 40, 5, 5, 24, 28, 12, 24, 38, 2...  \n",
              "1993  [9, 11, 2, 16, 11, 12, 6, 11, 2, 5, 5, 7, 5, 3...  \n",
              "1994  [31, 12, 4, 14, 15, 15, 24, 5, 5, 5, 32, 20, 3...  \n",
              "1995  [5, 5, 5, 32, 9, 28, 2, 2, 6, 2, 12, 24, 23, 2...  \n",
              "1996  [12, 15, 6, 32, 11, 2, 22, 11, 6, 9, 2, 6, 12,...  \n",
              "1997  [5, 5, 5, 32, 20, 32, 31, 38, 28, 28, 11, 2, 6...  \n",
              "1998  [9, 2, 11, 2, 6, 9, 5, 5, 3, 23, 11, 12, 6, 9,...  \n",
              "1999  [5, 5, 32, 28, 12, 6, 31, 5, 2, 6, 11, 12, 16,...  \n",
              "2000  [5, 5, 32, 9, 28, 2, 2, 6, 2, 2, 24, 6, 15, 2,...  \n",
              "2001  [5, 5, 5, 24, 28, 24, 22, 11, 2, 2, 16, 2, 11,...  \n",
              "2002  [5, 5, 5, 24, 2, 2, 16, 2, 11, 2, 6, 9, 2, 12,...  \n",
              "2003  [5, 5, 5, 24, 2, 6, 2, 2, 2, 24, 32, 15, 4, 9,...  \n",
              "2004  [28, 23, 2, 12, 32, 5, 5, 3, 22, 14, 9, 11, 2,...  \n",
              "2005  [5, 7, 5, 32, 20, 32, 9, 11, 2, 4, 14, 28, 2, ...  "
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.drop('split',axis=1)\n",
        "df_val.drop('split',axis=1)\n",
        "df_test.drop('split',axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VdIMnApWK6R"
      },
      "source": [
        "#Create a baseline model, using a simple neural architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "eEt00nE8j38I"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'tensorflow.keras' has no attribute 'Model'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13024/1476215417.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# base class model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_vocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mword_voc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.keras' has no attribute 'Model'"
          ]
        }
      ],
      "source": [
        "# base class model\n",
        "class RNN(tf.keras.Model):\n",
        "    def __init__(self, tag_vocab_size, sequence_len, embedding_matrix, batch_size):\n",
        "        super(RNN, self).__init__()\n",
        "        word_voc = embedding_matrix.shape[0]\n",
        "        embedding_dim = embedding_matrix.shape[1]\n",
        "\n",
        "        # input and embedding layers are shared among all the models\n",
        "        self.input_layer = Input(batch_input_shape=(batch_size, sequence_len))\n",
        "        self.embedding_layer = Embedding(word_voc, embedding_dim,weights=[embedding_matrix], trainable=False,mask_zero=True)\n",
        "\n",
        "\n",
        "    def build(self):\n",
        "        return Model(inputs=[self.input_layer], outputs=self(self.input_layer))\n",
        "\n",
        "    def copy_weights(self, model):\n",
        "        assert type(model).__name__ == type(self).__name__\n",
        "        for layer, model_layer in zip(self.layers[1:], model.layers[1:]):\n",
        "            layer.set_weights(model_layer.get_weights())\n",
        "\n",
        "    def loss_function(self, y_true, predictions):\n",
        "        mask = tf.math.logical_not(tf.math.equal(y, p2i[PAD])) # pad mask\n",
        "        sce = tf.keras.losses.sparse_categorical_crossentropy(y_true, predictions, from_logits=False)\n",
        "        mask = tf.cast(mask, dtype=sce.dtype)\n",
        "        sce *= mask\n",
        "        return tf.reduce_sum(sce)/tf.reduce_sum(mask)\n",
        "\n",
        "    def update_metrics(self, y_true, predictions, loss_obj, acc_obj, f1_obj):\n",
        "\n",
        "        # loss\n",
        "        current_loss = self.loss_function(y_true, predictions)\n",
        "        loss_obj.update_state(current_loss)\n",
        "\n",
        "        p1 = tf.math.logical_or(tf.math.equal(y_true, p2i[PAD]),tf.math.equal(y, p2i['.']))\n",
        "        p2 = tf.math.logical_or(tf.math.equal(y_true, p2i[',']),tf.math.equal(y, p2i[':']))\n",
        "        punctuation = tf.math.logical_or(p1, p2)\n",
        "        punctuation = tf.math.equal(y, p2i[PAD])\n",
        "        sample_weights = tf.math.logical_not(punctuation)\n",
        "        pred_np = np.argmax(predictions, axis=2)\n",
        "\n",
        "        # accuracy\n",
        "        acc_obj.update_state(y_true, pred_np, sample_weight=sample_weights)\n",
        "\n",
        "        # f1 macro\n",
        "        y_true_np = y.numpy().astype(np.int32)\n",
        "        f1 = f1_score(y_true_np.flatten(), pred_np.flatten(), sample_weight=tf.reshape(sample_weights,[-1]),average='macro', zero_division=0, labels=[*p2i.values()])\n",
        "        f1_obj.update_state(f1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0yqwzuvoTUF"
      },
      "outputs": [],
      "source": [
        "# bidirectional lstm + dense\n",
        "\n",
        "class BiLSTM(RNN):\n",
        "    def __init__(self, tag_vocab_size, sequence_len, embedding_matrix, batch_size, lstm_dim):\n",
        "        super(BiLSTM, self).__init__(tag_vocab_size, sequence_len, embedding_matrix, batch_size)\n",
        "        self.lstm = Bidirectional(LSTM(lstm_dim, return_state=True, return_sequences=True))\n",
        "        self.fc = Dense(tag_vocab_size, activation='softmax')\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        embedded = self.embedding_layer(x)\n",
        "        lstm_o, lstmf_h, lstmf_c, lstmb_h, lstmb_c = self.lstm(embedded)\n",
        "        output = self.fc(lstm_o)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngCRUgxcsS4q"
      },
      "outputs": [],
      "source": [
        "#GRU Model\n",
        "class GRU(RNN):\n",
        "    def __init__(self, tag_vocab_size, \n",
        "                 sequence_len, embedding_matrix, batch_size, gru_dim):\n",
        "        super(GRU, self).__init__(tag_vocab_size, sequence_len, embedding_matrix, batch_size)\n",
        "        self.gru = GRU(gru_dim, return_state=True, return_sequences=True)\n",
        "        self.fc = Dense(tag_vocab_size, activation='softmax')\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        embedded = self.embedding_layer(x)\n",
        "        gru_o, gru_sf, gru_sb = self.gru(embedded)\n",
        "        print(gru_o,gru_sf,gru_sb)\n",
        "        output = self.fc(gru_o)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quBr5pu3oXA8"
      },
      "outputs": [],
      "source": [
        "#BiGRU\n",
        "\n",
        "# bidirectional lstm + dense ###################################################\n",
        "class BiGRU(RNN):\n",
        "    def __init__(self, tag_vocab_size, \n",
        "                 sequence_len, embedding_matrix, batch_size, gru_dim):\n",
        "        super(BiGRU, self).__init__(tag_vocab_size, sequence_len, embedding_matrix, batch_size)\n",
        "        self.gru = Bidirectional(GRU(gru_dim, return_state=True, return_sequences=True))\n",
        "        self.fc = Dense(tag_vocab_size, activation='softmax')\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        embedded = self.embedding_layer(x)\n",
        "        gru_o, gru_sf, gru_sb = self.gru(embedded)\n",
        "        output = self.fc(gru_o)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSeJ9AwhoaXi"
      },
      "outputs": [],
      "source": [
        "# bidirectional lstm + bidirectional lstm ######################################\n",
        "class BiLSTM_BiLSTM(RNN): \n",
        "    def __init__(self, tag_vocab_size, sequence_len, embedding_matrix, batch_size, lstm_dim):\n",
        "        super(BiLSTM_BiLSTM, self).__init__(tag_vocab_size, sequence_len, embedding_matrix, batch_size)\n",
        "        self.lstm1 = Bidirectional(LSTM(\n",
        "            lstm_dim, return_state=True, return_sequences=True))\n",
        "        self.lstm2 = Bidirectional(LSTM(\n",
        "            lstm_dim, return_state=True, return_sequences=True))\n",
        "        self.fc = Dense(tag_vocab_size, activation='softmax')\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        embedded = self.embedding_layer(x)\n",
        "        lstm_o, lstmf_h, lstmf_c, lstmb_h, lstmb_c = self.lstm1(embedded)\n",
        "        lstm_o, lstmf_h, lstmf_c, lstmb_h, lstmb_c = self.lstm2(lstm_o)\n",
        "        output = self.fc(lstm_o)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fyZdk2Dl3uj"
      },
      "outputs": [],
      "source": [
        "# helper class to iterate the data\n",
        "class DataIterator:\n",
        "    def __init__(self, df, sequence_len, batch_size):\n",
        "\n",
        "        #converting the tokens and tags to numpy\n",
        "        self.X = df['tokens'].to_numpy()\n",
        "        self.Y = df['tags'].to_numpy()\n",
        "\n",
        "        #checking if the length of tokens and tags are equal\n",
        "        assert self.X.shape[0] == self.Y.shape[0]\n",
        "        self.num_sentence = self.X.shape[0]\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "\n",
        "        # pad sentences\n",
        "        self.data_list = []\n",
        "        for i in range(self.num_sentence):\n",
        "            m = np.zeros((sequence_len))\n",
        "            n = np.zeros(sequence_len) + p2i[PAD]\n",
        "            l = min(len(self.X[i]), sequence_len)\n",
        "            m[:l] = self.X[i][:l]\n",
        "            n[:l] = self.Y[i][:l]\n",
        "            self.data_list.append((m, n))\n",
        "        self.shuffle()\n",
        "\n",
        "\n",
        "    def shuffle(self):\n",
        "        self.current = 0\n",
        "        random.shuffle(self.data_list)\n",
        "        # batch the data\n",
        "        num_batches = math.ceil(self.num_sentence/self.batch_size)\n",
        "        self.batches_x = []\n",
        "        self.batches_y = []\n",
        "        for i in range(num_batches):\n",
        "            batchx = []\n",
        "            batchy = []\n",
        "            for j in range(self.batch_size):\n",
        "                if i*batch_size+j >= self.num_sentence:\n",
        "                    break\n",
        "                batchx.append(self.data_list[i*self.batch_size+j][0])\n",
        "                batchy.append(self.data_list[i*self.batch_size+j][1])\n",
        "            self.batches_x.append(np.array(batchx))\n",
        "            self.batches_y.append(np.array(batchy))\n",
        "\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.current >= len(self.batches_x):\n",
        "            raise StopIteration\n",
        "        x = self.batches_x[self.current]\n",
        "        y = self.batches_y[self.current]\n",
        "        x = tf.cast(x, tf.float32)\n",
        "        y = tf.cast(y, tf.float32)\n",
        "        self.current += 1\n",
        "        return x,y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWpWari27Jg5"
      },
      "outputs": [],
      "source": [
        "def train_step_graph_fn(optimizer, model, x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x)\n",
        "        loss = model.loss_function(y, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def val_step_graph_fn(model, x):\n",
        "    predictions = model(x)\n",
        "    return predictions   \n",
        "\n",
        "# hyperparameters\n",
        "sequence_len = 128\n",
        "batch_size =   64\n",
        "max_epochs = 100\n",
        "learning_rate = 5e-3 \n",
        "latent_dim =   64\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5g_dtCIOcp9"
      },
      "source": [
        "#BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_FZc8Ntwdh0"
      },
      "outputs": [],
      "source": [
        "\n",
        "#pass the model which we want to run #BiLSTM\n",
        "RNNModel = BiGRU\n",
        "\n",
        "pos_vocab_size = len(p2i.keys())\n",
        "\n",
        "\n",
        "# train model\n",
        "model = RNNModel(pos_vocab_size, sequence_len, embedding_matrix_v2, batch_size, latent_dim)\n",
        "model.build().summary()\n",
        "\n",
        "# val model\n",
        "val_model = RNNModel(pos_vocab_size, sequence_len, embedding_matrix_v3, batch_size, latent_dim)\n",
        "val_model.build()\n",
        "\n",
        "\n",
        "# best model\n",
        "best_model = RNNModel(pos_vocab_size, sequence_len, embedding_matrix_v3, batch_size, latent_dim)\n",
        "best_model.build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Wfyo6xWMV3N"
      },
      "outputs": [],
      "source": [
        "# Plot confusion matrix on all test set\n",
        "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def confusion_matrix(y_acc,pred_acc):\n",
        "  cm = confusion_matrix(y_acc, pred_acc)[1:,1:]\n",
        "  cm = cm.astype('int')\n",
        "  print(i2p[y])\n",
        "  print(y_acc)\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=[i2p[y] for y in list(set(y_acc))][1:]) \n",
        "\n",
        "  fig = plt.figure(figsize=(25,25))\n",
        "  ax = fig.add_subplot(111)\n",
        "  disp.plot(include_values=True, ax=ax,values_format='.10g', cmap='inferno');\n",
        "  # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQf9RIkFpM5l"
      },
      "outputs": [],
      "source": [
        "# metrics\n",
        "import random \n",
        "import math\n",
        "\n",
        "train_loss_obj = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_f1_obj = tf.keras.metrics.Mean(name='train_f1')\n",
        "train_acc_obj = tf.keras.metrics.Accuracy(name='train_accuracy')\n",
        "val_loss_obj = tf.keras.metrics.Mean(name='val_loss')\n",
        "val_f1_obj = tf.keras.metrics.Mean(name='val_f1')\n",
        "val_acc_obj = tf.keras.metrics.Accuracy(name='val_accuracy')\n",
        "\n",
        "# optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_data = DataIterator(df_train, sequence_len, batch_size)\n",
        "val_data = DataIterator(df_val, sequence_len, batch_size)\n",
        "\n",
        "\n",
        "# train loop\n",
        "max_val_loss = math.inf\n",
        "not_improving = 0\n",
        "max_iter_not_improv = 2\n",
        "\n",
        "errors = []\n",
        "for epoch in range(max_epochs):\n",
        "    train_loss_obj.reset_states()\n",
        "    train_f1_obj.reset_states()\n",
        "    train_acc_obj.reset_states()\n",
        "\n",
        "    train_data.shuffle()\n",
        "    errors = errors[:-10]\n",
        "    for x,y in train_data:\n",
        "        predictions = train_step_graph_fn(optimizer, model, x, y)\n",
        "        #errors.append((x[np.argmax(predictions, axis = -1) != y], y, predictions))\n",
        "        model.update_metrics(y, predictions, train_loss_obj, train_acc_obj, train_f1_obj)\n",
        "\n",
        "    print(\"{}.  \\t[TRAINING]\\t  loss: {}  \\t accuracy: {} \\t f1-macro: {}\".format(epoch, train_loss_obj.result(),train_acc_obj.result(),train_f1_obj.result()))\n",
        "\n",
        "    # validation\n",
        "    if epoch%5 == 4:\n",
        "        val_loss_obj.reset_states()\n",
        "        val_acc_obj.reset_states()\n",
        "        val_f1_obj.reset_states()\n",
        "\n",
        "        val_model.copy_weights(model)\n",
        "        val_data.shuffle()\n",
        "        for x,y in val_data:\n",
        "            predictions = val_step_graph_fn(val_model, x)\n",
        "            val_model.update_metrics(y, predictions, val_loss_obj, val_acc_obj, val_f1_obj)\n",
        "\n",
        "        print(\"     \\t[VALIDATION]\\t   loss: {}  \\t  accuracy: {} \\t  f1-macro: {}\".format( val_loss_obj.result(),val_acc_obj.result(),val_f1_obj.result()))\n",
        "\n",
        "        # early stopping\n",
        "        if val_loss_obj.result() < max_val_loss:\n",
        "            best_model.copy_weights(val_model)\n",
        "            max_val_loss = val_loss_obj.result()\n",
        "            not_improving = 0\n",
        "        else:\n",
        "            not_improving += 1\n",
        "            print(\"VALIDATION LOSS NOT IMPROVING, STRIKE:\", not_improving,\"!!\")\n",
        "            if not_improving >= max_iter_not_improv:\n",
        "                print(\"Validation loss not improving for\", max_iter_not_improv,\n",
        "                      \"successive computations.\")\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHIM6iw75zNB"
      },
      "outputs": [],
      "source": [
        "test_seq_len = 1500 \n",
        "test_model = RNNModel(pos_vocab_size, test_seq_len, embedding_matrix_v4, batch_size, latent_dim)\n",
        "test_model.build().summary()\n",
        "test_model.copy_weights(best_model)\n",
        "\n",
        "# run on the test set\n",
        "test_loss_obj = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_f1_obj = tf.keras.metrics.Mean(name='test_f1')\n",
        "test_acc_obj = tf.keras.metrics.Accuracy(name='test_accuracy')\n",
        "\n",
        "test_data = DataIterator(df_test, test_seq_len, batch_size)\n",
        "\n",
        "y_acc, pred_acc = [], []\n",
        "for x, y in test_data:\n",
        "    predictions = val_step_graph_fn(test_model, x)\n",
        "    test_model.update_metrics(y, predictions, test_loss_obj, test_acc_obj, test_f1_obj)\n",
        "    y_acc += list(y.numpy().flatten())\n",
        "    if type(predictions) is tuple:\n",
        "        pred_acc += list(predictions[0].numpy().flatten())\n",
        "    else:                         # other models\n",
        "        pred_acc += list(np.argmax(predictions, axis=2).flatten())\n",
        "\n",
        "test_log = \"\\nTEST loss: {}  \\t accuracy: {} \\t f1-macro: {}\\n\".format(test_loss_obj.result(),test_acc_obj.result(),test_f1_obj.result())\n",
        "\n",
        "print(test_log)\n",
        "confusion_matrix(y_acc,pred_acc) "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NPL_25_11.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
